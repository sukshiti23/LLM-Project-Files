{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3098ea3-cdfe-46ca-acae-14208d6b9511",
   "metadata": {},
   "source": [
    "# **TOKENIZATION**\n",
    "# What is Tokenization? \n",
    "# Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking down a stream of text into smaller units called tokens.\n",
    "(Source: GeeksforGeeks)\n",
    "\n",
    "\n",
    "*Data (Book) : The Wealth of Nations by Adam Smith*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c613295-d207-49d0-9726-e2b765be3bf5",
   "metadata": {},
   "source": [
    "**Loading & Reading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98fca64a-4873-41c1-b2c7-ce5a7788d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('The-Wealth-of-Nations.txt', encoding='utf-8') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8c1073b-16e4-49e0-8e05-910084bdf32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2248172\n"
     ]
    }
   ],
   "source": [
    "# Number of characters in the text\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d87b82-4493-4f1a-bffa-534787f41d96",
   "metadata": {},
   "source": [
    "The data has more than 22 Lakh characters but not all characters are unique in this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17050d-671b-465a-8ed6-860d38bc5bdf",
   "metadata": {},
   "source": [
    "**Using the Regular Expressions to split the data using patterns. We have taken a book data which has punctuations. We keep the punctuations as the context of a text changes because of a punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a96bdd7-4a7c-4496-90e5-6753e33baf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the regular expression library i.e re\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83dd26de-f79b-4ed1-98ed-1c1e6a80d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the text data using the patterns using the re library\n",
    "preprocessed = re.split(r'([,.!:;?_\"()\\']|--|\\s)', raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da053a03-dfa0-496b-b897-fc21879920b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " ' ',\n",
       " 'Inquiry',\n",
       " ' ',\n",
       " 'into',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Nature',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'Causes',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Wealth',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'Nations',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'Adam',\n",
       " ' ',\n",
       " 'Smith',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'publication',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'The',\n",
       " '\\n',\n",
       " 'Electronic',\n",
       " ' ',\n",
       " 'Classics',\n",
       " ' ',\n",
       " 'Series',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'This',\n",
       " ' ',\n",
       " 'Portable',\n",
       " ' ',\n",
       " 'Document',\n",
       " ' ',\n",
       " 'file',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'furnished',\n",
       " ' ',\n",
       " 'free',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'without',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'charge',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'any',\n",
       " '\\n',\n",
       " 'kind',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'Any',\n",
       " ' ',\n",
       " 'person',\n",
       " ' ',\n",
       " 'using',\n",
       " ' ',\n",
       " 'this',\n",
       " ' ',\n",
       " 'document',\n",
       " ' ',\n",
       " 'file',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'for',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'purpose',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'way',\n",
       " ' ',\n",
       " 'does',\n",
       " ' ',\n",
       " 'so',\n",
       " ' ',\n",
       " 'at',\n",
       " ' ',\n",
       " 'his',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'her',\n",
       " ' ',\n",
       " 'own',\n",
       " '\\n',\n",
       " 'risk',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'Neither',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Pennsylvania',\n",
       " ' ',\n",
       " 'State',\n",
       " ' ',\n",
       " 'University',\n",
       " ' ',\n",
       " 'nor',\n",
       " ' ',\n",
       " 'Jim',\n",
       " ' ',\n",
       " 'Manis',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'Editor',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'nor',\n",
       " ' ',\n",
       " 'anyone',\n",
       " ' ',\n",
       " 'associated',\n",
       " ' ',\n",
       " 'with',\n",
       " ' ',\n",
       " 'the',\n",
       " '\\n',\n",
       " 'Pennsylvania',\n",
       " ' ',\n",
       " 'State',\n",
       " ' ',\n",
       " 'University',\n",
       " ' ',\n",
       " 'assumes',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'responsibility',\n",
       " ' ',\n",
       " 'for',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'material',\n",
       " ' ',\n",
       " 'contained',\n",
       " ' ',\n",
       " 'within',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'document',\n",
       " '\\n',\n",
       " 'or',\n",
       " ' ',\n",
       " 'for',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'file',\n",
       " ' ',\n",
       " 'as',\n",
       " ' ',\n",
       " 'an',\n",
       " ' ',\n",
       " 'electronic',\n",
       " ' ',\n",
       " 'transmission',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'way',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'An',\n",
       " ' ',\n",
       " 'Inquiry',\n",
       " ' ',\n",
       " 'into',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Nature',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'Causes',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Wealth',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'Nations',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'Adam',\n",
       " ' ',\n",
       " 'Smith',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'The',\n",
       " ' ',\n",
       " 'Electronic',\n",
       " ' ',\n",
       " 'Classics',\n",
       " '\\n',\n",
       " 'Series',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'Jim',\n",
       " ' ',\n",
       " 'Manis',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'Editor',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'PSU-Hazleton',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'Hazleton',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'PA',\n",
       " ' ',\n",
       " '18202',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'Portable',\n",
       " ' ',\n",
       " 'Document',\n",
       " ' ',\n",
       " 'File',\n",
       " ' ',\n",
       " 'produced',\n",
       " ' ',\n",
       " 'as',\n",
       " '\\n',\n",
       " 'part',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'an',\n",
       " ' ',\n",
       " 'ongoing',\n",
       " ' ',\n",
       " 'publication',\n",
       " ' ',\n",
       " 'project',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'bring',\n",
       " ' ',\n",
       " 'classical',\n",
       " ' ',\n",
       " 'works',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'literature',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'English',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'free',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'easy',\n",
       " '\\n',\n",
       " 'access',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'those',\n",
       " ' ',\n",
       " 'wishing',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'make',\n",
       " ' ',\n",
       " 'use',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'them',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'Jim',\n",
       " ' ',\n",
       " 'Manis',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'faculty',\n",
       " ' ',\n",
       " 'member',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'English',\n",
       " ' ',\n",
       " 'Department',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'The',\n",
       " ' ',\n",
       " 'Pennsylvania',\n",
       " ' ',\n",
       " 'State',\n",
       " ' ',\n",
       " 'University',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'This',\n",
       " ' ',\n",
       " 'page',\n",
       " '\\n',\n",
       " 'and',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'preceding',\n",
       " ' ',\n",
       " 'page',\n",
       " '(',\n",
       " 's',\n",
       " ')',\n",
       " '',\n",
       " ' ',\n",
       " 'are',\n",
       " ' ',\n",
       " 'restricted',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'copyright',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'The',\n",
       " ' ',\n",
       " 'text',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'following',\n",
       " ' ',\n",
       " 'pages',\n",
       " ' ',\n",
       " 'are',\n",
       " ' ',\n",
       " 'not',\n",
       " ' ',\n",
       " 'copyrighted',\n",
       " '\\n',\n",
       " 'within',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'United',\n",
       " ' ',\n",
       " 'States',\n",
       " ';',\n",
       " '',\n",
       " ' ',\n",
       " 'however',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'fonts',\n",
       " ' ',\n",
       " 'used',\n",
       " ' ',\n",
       " 'may',\n",
       " ' ',\n",
       " 'be',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'Cover',\n",
       " ' ',\n",
       " 'Design',\n",
       " ':',\n",
       " '',\n",
       " ' ',\n",
       " 'Jim',\n",
       " ' ',\n",
       " 'Manis',\n",
       " '\\n',\n",
       " 'Copyright',\n",
       " ' ',\n",
       " '©',\n",
       " ' ',\n",
       " '2005',\n",
       " '\\n',\n",
       " 'The',\n",
       " ' ',\n",
       " 'Pennsylvania',\n",
       " ' ',\n",
       " 'State',\n",
       " ' ',\n",
       " 'University',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'an',\n",
       " ' ',\n",
       " 'equal',\n",
       " ' ',\n",
       " 'opportunity',\n",
       " ' ',\n",
       " 'university',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'I',\n",
       " ' ',\n",
       " 'have',\n",
       " ' ',\n",
       " 'endeavoured',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'fourth',\n",
       " ' ',\n",
       " 'book',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'explain',\n",
       " ' ',\n",
       " 'as',\n",
       " ' ',\n",
       " 'fully',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'distinctly',\n",
       " ' ',\n",
       " 'as',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'can',\n",
       " ' ',\n",
       " 'those',\n",
       " ' ',\n",
       " 'different',\n",
       " ' ',\n",
       " 'theories',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'principal',\n",
       " ' ',\n",
       " 'effects',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'CAUSES',\n",
       " ' ',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'IMPROVEMENT',\n",
       " ' ',\n",
       " 'IN',\n",
       " '\\n',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'PRODUCTIVE',\n",
       " ' ',\n",
       " 'POWERS',\n",
       " ' ',\n",
       " 'OF',\n",
       " '\\n',\n",
       " 'L',\n",
       " ' ',\n",
       " 'ABOUR',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'AND',\n",
       " ' ',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'ORDER',\n",
       " ' ',\n",
       " 'ACCORDING',\n",
       " ' ',\n",
       " 'TO',\n",
       " ' ',\n",
       " 'WHICH',\n",
       " ' ',\n",
       " 'ITS',\n",
       " ' ',\n",
       " 'PRODUCE',\n",
       " ' ',\n",
       " 'IS',\n",
       " '\\n',\n",
       " 'NATURALLY',\n",
       " ' ',\n",
       " 'DISTRIBUTED',\n",
       " ' ',\n",
       " 'AMONG',\n",
       " '\\n',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'DIFFERENT',\n",
       " ' ',\n",
       " 'RANKS',\n",
       " ' ',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'THE',\n",
       " '\\n',\n",
       " 'PEOPLE',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'which',\n",
       " ' ',\n",
       " 'they',\n",
       " ' ',\n",
       " 'have',\n",
       " ' ',\n",
       " 'produced',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'different',\n",
       " ' ',\n",
       " 'ages',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'nations',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'To',\n",
       " ' ',\n",
       " 'explain',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'what',\n",
       " ' ',\n",
       " 'has',\n",
       " ' ',\n",
       " 'consisted',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'revenue',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'great',\n",
       " ' ',\n",
       " 'body',\n",
       " '\\n',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'people',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'what',\n",
       " ' ',\n",
       " 'has',\n",
       " ' ',\n",
       " 'been',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'nature',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'those',\n",
       " ' ',\n",
       " 'funds',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'which',\n",
       " ',',\n",
       " '',\n",
       " '\\n',\n",
       " 'in',\n",
       " ' ',\n",
       " 'different',\n",
       " ' ',\n",
       " 'ages',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'nations',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'have',\n",
       " ' ',\n",
       " 'supplied',\n",
       " ' ',\n",
       " 'their',\n",
       " ' ',\n",
       " 'annual',\n",
       " ' ',\n",
       " 'consumption',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'object',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'these',\n",
       " ' ',\n",
       " 'four',\n",
       " ' ',\n",
       " 'first',\n",
       " ' ',\n",
       " 'books',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'The',\n",
       " ' ',\n",
       " 'fifth',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'last',\n",
       " ' ',\n",
       " 'book',\n",
       " '\\n',\n",
       " 'treats',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'revenue',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'sovereign',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'commonwealth',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'In',\n",
       " ' ',\n",
       " 'this',\n",
       " '\\n',\n",
       " 'book',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'have',\n",
       " ' ',\n",
       " 'endeavoured',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'shew',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'first',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'what',\n",
       " ' ',\n",
       " 'are',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'necessary',\n",
       " '\\n',\n",
       " 'expenses',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'sovereign',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'commonwealth',\n",
       " ';',\n",
       " '',\n",
       " ' ',\n",
       " 'which',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'those',\n",
       " ' ',\n",
       " 'ex-',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'CHAPTER',\n",
       " ' ',\n",
       " 'I',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'penses',\n",
       " ' ',\n",
       " 'ought',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'be',\n",
       " ' ',\n",
       " 'defrayed',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'general',\n",
       " ' ',\n",
       " 'contribution',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " '\\n',\n",
       " 'whole',\n",
       " ' ',\n",
       " 'society',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'which',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'them',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'that',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'some',\n",
       " ' ',\n",
       " 'particular',\n",
       " ' ',\n",
       " 'part',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'DIVISION',\n",
       " ' ',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'LABOUR',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'only',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'some',\n",
       " ' ',\n",
       " 'particular',\n",
       " ' ',\n",
       " 'members',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'it',\n",
       " ':',\n",
       " '',\n",
       " ' ',\n",
       " 'secondly',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'what',\n",
       " ' ',\n",
       " 'are',\n",
       " ' ',\n",
       " 'the',\n",
       " '\\n',\n",
       " 'different',\n",
       " ' ',\n",
       " 'methods',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'which',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'whole',\n",
       " ' ',\n",
       " 'society',\n",
       " ' ',\n",
       " 'may',\n",
       " ' ',\n",
       " 'be',\n",
       " ' ',\n",
       " 'made',\n",
       " ' ',\n",
       " 'to',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'T',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'HE',\n",
       " ' ',\n",
       " 'GREATEST',\n",
       " ' ',\n",
       " 'IMPROVEMENTS',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'productive',\n",
       " ' ',\n",
       " 'powers',\n",
       " ' ',\n",
       " 'of',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'labour',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'greater',\n",
       " ' ',\n",
       " 'part',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'skill',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'dexterity',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " '\\n',\n",
       " 'judgment',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'with',\n",
       " ' ',\n",
       " 'which',\n",
       " ' ',\n",
       " 'it',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'anywhere',\n",
       " ' ',\n",
       " ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d389a4-29ac-4f5e-bf8d-af8de75bdb4b",
   "metadata": {},
   "source": [
    "From the above output, we can see that it contains white spaces too!\n",
    "\n",
    "We want to remove the white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "909bc53f-ddbe-4fd3-9308-8f2ac9e0a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = [item for item in preprocessed if item.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9292d01-3263-4e07-a324-6446ae0aa445",
   "metadata": {},
   "source": [
    "Tokens i.e words have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa3dd76f-8b02-4ebb-b969-33a11284ed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " 'Inquiry',\n",
       " 'into',\n",
       " 'the',\n",
       " 'Nature',\n",
       " 'and',\n",
       " 'Causes',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Wealth',\n",
       " 'of',\n",
       " 'Nations',\n",
       " 'by',\n",
       " 'Adam',\n",
       " 'Smith',\n",
       " 'is',\n",
       " 'a',\n",
       " 'publication',\n",
       " 'of',\n",
       " 'The']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f8a3e-2991-4e68-bf3f-f66644b8d7be",
   "metadata": {},
   "source": [
    "Thus, we successfully removed the white spaces from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c749cb7-ee4a-49b9-89b8-73ca167d4876",
   "metadata": {},
   "source": [
    "# Vocabulary: In the context of Large Language Models (LLMs), the vocabulary is the set of all tokens (words, subwords, or characters) that the model knows and can understand.\n",
    "\n",
    "We create a vocabulary of the given data. So that when the model is trained and used it knows which words to use.\n",
    "\n",
    "Vocabulary is a dictionary of words with proper IDs.\n",
    "\n",
    "Thus, the words in the vocabulary should be unique and not repeated.\n",
    "\n",
    "\n",
    "**Creating vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "486424a7-06e0-45e8-9f7d-771cfc20885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: get all unique words sorted alphabetically\n",
    "unique_words = sorted(set(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb60c1dc-0db7-4b9d-b7dd-4d0c560cc321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13984"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(unique_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4bca1-aab9-44be-9bce-1f5f77b30afd",
   "metadata": {},
   "source": [
    "So, from the 22 Lakh words we have only 13,984 unique words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7140c-a18c-41b8-8757-1178b4a86bb9",
   "metadata": {},
   "source": [
    "**Now, we create a *Vocabulary* and assign unique *TokenIDs* to the tokens(here, words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0af8b03-96c5-476a-935b-6d0e5df83c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c2629-8088-4019-98da-37b7c26ab7e0",
   "metadata": {},
   "source": [
    "We swap the token and integer to create a tokenID (integer) for that specific token (token).\n",
    "\n",
    "*enumerate()* function aloops through all elements and returns both value and index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a155a87-db4b-4384-a294-3820ba528389",
   "metadata": {},
   "source": [
    "A Vocabulary is a dictionary which provides mapping from token to token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b66c448d-f3fb-4994-a67b-4a451a63d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('#', 1)\n",
      "('(', 2)\n",
      "(')', 3)\n",
      "(',', 4)\n",
      "('-', 5)\n",
      "('.', 6)\n",
      "('/', 7)\n",
      "('//www', 8)\n",
      "('0', 9)\n",
      "('000', 10)\n",
      "('001', 11)\n",
      "('017', 12)\n",
      "('023', 13)\n",
      "('027', 14)\n",
      "('029', 15)\n",
      "('041', 16)\n",
      "('055', 17)\n",
      "('068', 18)\n",
      "('075', 19)\n",
      "('076', 20)\n",
      "('083', 21)\n",
      "('086', 22)\n",
      "('092', 23)\n",
      "('0d', 24)\n",
      "('0¹/³', 25)\n",
      "('0¼', 26)\n",
      "('0½', 27)\n",
      "('0¾', 28)\n",
      "('0¾d', 29)\n",
      "('1', 30)\n",
      "('1-4th', 31)\n",
      "('1/', 32)\n",
      "('1/12d', 33)\n",
      "('1/2', 34)\n",
      "('1/3', 35)\n",
      "('1/3d', 36)\n",
      "('1/6', 37)\n",
      "('10', 38)\n",
      "('10/32', 39)\n",
      "('100', 40)\n",
      "('1000', 41)\n",
      "('101', 42)\n",
      "('102', 43)\n",
      "('103', 44)\n",
      "('104', 45)\n",
      "('105', 46)\n",
      "('106', 47)\n",
      "('107', 48)\n",
      "('108', 49)\n"
     ]
    }
   ],
   "source": [
    "# displaying some part of the vocabulary\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    if(i<50):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc617b-65bb-4883-a320-fa695af3055d",
   "metadata": {},
   "source": [
    "# This process can be called as *Encoding*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a95f1-38e4-4654-86e9-101c7b96e472",
   "metadata": {},
   "source": [
    "In simple terms, \n",
    "\n",
    "**Encoding** means turning text (like words or sentences) into numbers (token IDs) that a computer or model can understand.\n",
    "\n",
    "**Decoding** means turning those numbers (token IDs) back into the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c7cef6-4a4d-464a-9f42-6c01cc47ef23",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee286b22-417b-4243-915f-65b0eb424ae0",
   "metadata": {},
   "source": [
    "We will now implement simple tokenization classes to encode & decode the text then, we will move on to *Types of Tokenizations* and *Byte-Pair Encoding*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c98a7-269a-4407-b359-203d8eddf921",
   "metadata": {},
   "source": [
    "# Tokenizer Class 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3feb8d04-cbee-4347-92ab-d6c6a91f0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer1:\n",
    "    def __init__(self, vocab): #init -> automatically called when an instance of a class is created\n",
    "        self.str_to_int = vocab \n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) #tokenized the text\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()] #removing white spaces\n",
    "\n",
    "        #creating token ids \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    # decoding the ids to get string\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) #reversing the method used for vocabulary \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', text) #replaced the punctuations\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a82d1-3b39-41f9-960b-c29818d55079",
   "metadata": {},
   "source": [
    "Testing the class by creating an instance of the Tokenizer1 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7aa936c-eb07-4019-94a0-c5ce9d09d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5abd815-0eed-4eb7-8046-1dd4b8b46883",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"countries which enjoy the highest degree of industry and improvement; what is the work of one man, in a rude state of society,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56c6124b-eb7b-43fc-9f6b-9dc1b7647276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5214, 13635, 6396, 12692, 7749, 5511, 9669, 8157, 3520, 8010, 1291, 13612, 8411, 12692, 13740, 9669, 9706, 8941, 4, 8025, 3092, 11471, 12182, 9669, 12010, 4]\n"
     ]
    }
   ],
   "source": [
    "# ENCODING\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b40cf20e-fa6e-4243-b2ca-a2793cfea832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries which enjoy the highest degree of industry and improvement ; what is the work of one man, in a rude state of society,\n"
     ]
    }
   ],
   "source": [
    "# DECODING THE IDs\n",
    "decode = tokenizer.decode(ids)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35563686-6a1b-4057-9dbf-c7f449a875c1",
   "metadata": {},
   "source": [
    "Thus, our Tokenizer1 class o working correctly.\n",
    "\n",
    "\n",
    "But the problem with this approach is - when an unknown words is passed / a word out of vocabulary is passed, it throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7d1dc98-d225-467d-bb8c-524d0bdf8964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# we can confirm by running this cell\\n\\ntext = \"Apple mobiles are good\\n# ENCODING\\nids = tokenizer.encode(text)\\nprint(ids)\\n# DECODING THE IDs\\ndecode = tokenizer.decode(ids)\\nprint(decode)\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# we can confirm by running this cell\n",
    "\n",
    "text = \"Apple mobiles are good\n",
    "# ENCODING\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "# DECODING THE IDs\n",
    "decode = tokenizer.decode(ids)\n",
    "print(decode)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1484a2-69cf-4507-87e2-24276e692439",
   "metadata": {},
   "source": [
    "To overcome this problem, we use **Special Context Tokens**\n",
    "\n",
    "**Special Context Tokens** can be used to deal with unknown words i.e <|unk|> token or if data is taken from multiple sources we use <|endoftext|> token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50218c87-3f45-43b2-a852-3716e719fcf7",
   "metadata": {},
   "source": [
    "**We create another Tokenizer class to incorporate these tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11802ed5-5453-4e19-95f1-f05ff7f73d5c",
   "metadata": {},
   "source": [
    "We will update the vocabulary by adding the *Special Context Tokens*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c8852718-7cb4-4cf1-8240-5d86653eef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "#adding the new tokens\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "\n",
    "#giving token IDs to the tokens & creating a vocabulary \n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a9012-6c14-4bce-ae3c-e4944ad961d1",
   "metadata": {},
   "source": [
    "Thus, the special context tokens have been added to the vocabulary.\n",
    "\n",
    "We can verify it by-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a102e72e-d152-4490-a072-eef57dd29f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13986"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510fc37c-3b44-4199-8608-4c393d93f6eb",
   "metadata": {},
   "source": [
    "As seen, prior to adding the special context tokens the vocab size was 13984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e223dc73-f8b4-4120-9d0c-338d3f667187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('”', 13983)\n",
      "('<|endoftext|>', 13984)\n",
      "('<|unk|>', 13985)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-3:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835e605-578c-4059-b627-d2fc26ef6d2c",
   "metadata": {},
   "source": [
    "Thus, the last two tokens are special context tokens.\n",
    "\n",
    "We successfully added the *Special Context Tokens*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e89e9b-eb63-4c5e-9100-507ff8d42887",
   "metadata": {},
   "source": [
    "Now we create the Tokenizer class which will encode depending on the type of token(unknown or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0e37f-4843-4084-ac74-ad134c978f77",
   "metadata": {},
   "source": [
    "# Tokenizer Class 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7cd554f-b43c-43b4-aec1-9c6aa591f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        #in encoding we will check if the given token belongs to the text data. if it doesn't the token will be <|unk|>\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        #checking if there are unknown words and assigning special context tokens\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        #if the item if in the vocabulary -> return the item else name it as <|unk|>\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids): #code remains the same\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1994f07c-04ce-4195-8810-69eb69b15b0c",
   "metadata": {},
   "source": [
    "Testing the Tokenizer2 class by creating as instance of the Tokenizer2 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "868ae045-fdea-4614-8963-bc8d08d8e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer2(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29726c0b-189f-4da3-b315-cc7e3195e41e",
   "metadata": {},
   "source": [
    "**Testing for <|unk|> token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96c21e7b-d860-4af1-9e58-5f0b10e938a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Thirdly, and lastly, everybody must be thoughtful(itisunknown) how much labour\n",
    "is facilitated and abridged by the application of proper machinery.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5295e6f9-8650-4872-b60b-8e6b47c5839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded => [2893, 4, 3520, 8594, 4, 6565, 9383, 3957, 13985, 2, 13985, 3, 7846, 9349, 8543, 8411, 6789, 3520, 3152, 4278, 12692, 3613, 9669, 10668, 8895, 6]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(f'Encoded => {ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46824326-cfda-4b24-8db4-73be8c6f0f25",
   "metadata": {},
   "source": [
    "The 13985  token indicates the <|unk|> token.\n",
    "\n",
    "We can decode it as-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbcd9341-5c1a-47a5-a571-634bc6c2f677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded => Thirdly, and lastly, everybody must be <|unk|>( <|unk|>) how much labour is facilitated and abridged by the application of proper machinery.\n"
     ]
    }
   ],
   "source": [
    "original_text = tokenizer.decode(ids)\n",
    "print(f'Decoded => {original_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721de79-8d68-4ee6-8551-ac52cc20c6f7",
   "metadata": {},
   "source": [
    "**Testing for <|endoftext|> token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "25185a83-346e-40f6-92d5-b8455adadac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"\"\"Thirdly, and lastly, everybody must be sensible how much labour\n",
    "is facilitated and abridged by the application of proper machinery.\"\"\"\n",
    "text_2 = \"\"\"It is unnecessary to give any example.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d9e7eda-42e1-44dc-9e3b-10aa48781a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thirdly, and lastly, everybody must be sensible how much labour\n",
      "is facilitated and abridged by the application of proper machinery.<|endoftext|>It is unnecessary to give any example.\n"
     ]
    }
   ],
   "source": [
    "text = \"<|endoftext|>\".join((text_1, text_2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e00c8-ff1d-48c6-867e-f7da6de773d5",
   "metadata": {},
   "source": [
    "We considered 2 text sources and joined them using <|endoftext|>\n",
    "\n",
    "In models like GPT, <|endoftext|> tokens are used since the data is taken from various different sources. The <|endoftext|> tokens indicates that the data sources are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6e211b8b-ff73-4af2-9ee5-206b4ef28152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded => [2893, 4, 3520, 8594, 4, 6565, 9383, 3957, 11699, 7846, 9349, 8543, 8411, 6789, 3520, 3152, 4278, 12692, 3613, 9669, 10668, 8895, 6, 13985, 8411, 13226, 12837, 7411, 3571, 6595, 6]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(f'Encoded => {ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eea3cfd6-f117-4c71-a241-7eb71a7eb817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded => Thirdly, and lastly, everybody must be sensible how much labour is facilitated and abridged by the application of proper machinery. <|unk|> is unnecessary to give any example.\n"
     ]
    }
   ],
   "source": [
    "original_text = tokenizer.decode(ids)\n",
    "print(f'Decoded => {original_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326644a4-0c2f-46a1-8051-ce8b2a2de21e",
   "metadata": {},
   "source": [
    "The token number 13985 indicates the <|endoftext|> token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447b8be-8043-4081-88a0-37846508c813",
   "metadata": {},
   "source": [
    "**Testing without any special tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d785bf9-2a4d-4474-b07b-8bd7112bb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Thirdly, and lastly, everybody must be sensible how much labour\n",
    "is facilitated and abridged by the application of proper machinery.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1fcb27d6-6081-4551-96b1-8c139d39052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded => [2893, 4, 3520, 8594, 4, 6565, 9383, 3957, 11699, 7846, 9349, 8543, 8411, 6789, 3520, 3152, 4278, 12692, 3613, 9669, 10668, 8895, 6]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(f'Encoded => {ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3093adae-36ed-4b6a-b00c-41643c3e9c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded => Thirdly, and lastly, everybody must be sensible how much labour is facilitated and abridged by the application of proper machinery.\n"
     ]
    }
   ],
   "source": [
    "original_text = tokenizer.decode(ids)\n",
    "print(f'Decoded => {original_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1ccea-05f0-4e42-9b55-2f5154944f6a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baced59e-d289-40a2-a1bd-e42ec1881e57",
   "metadata": {},
   "source": [
    "# Additional Tokens\n",
    "\n",
    "# 1) BOS (Beginning of Sequence) - signifies to LLM where a content begins\n",
    "# 2) EOS (End of Sequence) - useful when concatenating texts at end of texts\n",
    "# 3) PAD (Padding) - to ensure texts in batches have same lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32eae4-3db4-40c0-9fd6-5160cdae7d62",
   "metadata": {},
   "source": [
    "# **Types of Tokenizations:** \n",
    "\n",
    "**1. Word-based - every word in a text is a token but it has OOV (out of vocabulary) problems**\n",
    "\n",
    "**2. Character-based - individual characters (letters) are considered as tokens but the meaning of a word is lost**\n",
    "\n",
    "**3. *Sub Word-based***\n",
    "\n",
    "it follows 2 rules:\n",
    "\n",
    "**Rule1 - don't split frequently used words into sub-words** \n",
    "\n",
    "**Rule2 - split rare words into smaller meaningful sub-words**\n",
    "\n",
    "**(helps model learn the different words with the common root word (eg. token, tokens, tokenizing) - helps model learn to differentiate between different words with same suffix)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b3418-5d4e-465c-ac48-a16738978912",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding(BPE)\n",
    "\n",
    "> Models like **GPT** use this type of encoding. It follows a *Sub-Word-based Tokenization*\n",
    "\n",
    "> In this, the most common words/characters are represented a single token while rare words are broken down into 2 or more tokens\n",
    "\n",
    "> In BPE, the data is checked in sequence and the common pair(byte-pair) that occurs is replaced by a byte (variable) that does NOT occur in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed475bfd-b890-4881-9c50-b962442a9996",
   "metadata": {},
   "source": [
    "# Implementing Byte-Pair Encoding\n",
    "\n",
    "> Tiktoken library is used which is a BPE library (used by OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d9856d2f-05ca-42e5-a6d1-75a3500063a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tiktoken if not installed\n",
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9830868-0300-4f54-8ca8-782e1268ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737de083-6dda-41c5-8ee5-690975bc7823",
   "metadata": {},
   "source": [
    "Instantiating BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8e6a19b2-db8f-4fd5-9fd9-6cedecf0573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0c2fd-fe4a-4016-9478-63faf49bebd4",
   "metadata": {},
   "source": [
    "We use rules of the GPT-2 model to tokenize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0dfd85a4-5df3-40e1-a285-6b71550c31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"But though this equality of treatment should not be productive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a79f5d-ef17-44a8-a5b9-a589033f0dee",
   "metadata": {},
   "source": [
    "Counting number of tokens this text has-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51976759-7a0d-444c-9155-6da0362dbecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Token count:\", len(bpe_tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3340080d-ed22-4254-85a3-b8ecbc3280d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1537, 996, 428, 10537, 286, 3513, 815, 407, 307, 12973]\n"
     ]
    }
   ],
   "source": [
    "ids = bpe_tokenizer.encode(text)\n",
    "print(f'Token IDs: {ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c67407b3-acdc-441b-a884-6a5e7e478dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: But though this equality of treatment should not be productive\n"
     ]
    }
   ],
   "source": [
    "original_text = bpe_tokenizer.decode(ids)\n",
    "print(f'Original Text: {original_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147bb38e-2229-4eb0-afa2-54aa12806caf",
   "metadata": {},
   "source": [
    "Thus, we successfully implemented amd tested the titoken's tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f93cc-af9c-4fe0-8ae9-569311230c83",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf16f7-83ad-4ecc-acf4-a89ed72ff817",
   "metadata": {},
   "source": [
    "A final step before creating Vector Embeddings is creating Input-Target pairs. An LLM is an ***Auto-Regressive*** model i.e the output of one iteration is the input of text iteration. It is a self-supervised learning or unsupervised learning.\n",
    "\n",
    "*The main task of an LLM is 'Next Word Prediction'.* The resulting extra tasks performed by the LLMs such as answering questions etc. this behaviour by the LLMs is called *Emergent Behaviour*, since the model wasn't train to perform these tasks in the first place.\n",
    "\n",
    "As seen from previous code, we are given a context i.e the size of the input. When given a data, the LLM isn't shown the entire text. It is only given an input of size = context. So, if input = context then according to the purpose of LLMs (*Next Word Prediction*), it should predict the output which is context+1. The LLMs cannot access the elements past the target. Then this predicted output along with the previous inputs are taken as input then it predicts the next word. This process goes on. \n",
    "\n",
    "\n",
    "**Creating Input-Target pairs using Data Loaders**\n",
    "\n",
    "*Dataloader* -> PyTorch Dataloader is a utility class designed to simplify loading and iterating over datasets while training deep learning models. (Source: GeeksforGeeks)\n",
    "\n",
    "*Sliding Window* ->  Sliding window problems are computational problems in which a fixed/variable-size window is moved through a data structure, typically an array or string. (Source: GeeksforGeeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2b9c5-9ec8-4958-8b0d-e9c0a3375f01",
   "metadata": {},
   "source": [
    "**Step 1: Importing the data & tokenizing the text using BPE Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "05e0e059-d13f-4101-a848-251ba75abf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tiktoken and creating an instance of the BPE tokenizer object\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69b250d1-3689-46f2-9d99-412fddb569a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('The-Wealth-of-Nations.txt','r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "#Tokenizing\n",
    "enc_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3520187c-f6d7-4b4c-8f30-c212f6a10199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527097\n"
     ]
    }
   ],
   "source": [
    "# length of the text\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ad14236-9a43-4085-b9eb-778f320f6553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025, 39138, 656, 262, 10362, 290, 46865, 286, 262, 35151, 286, 7973, 416, 7244, 4176, 318, 257, 9207, 286, 383]\n"
     ]
    }
   ],
   "source": [
    "print(enc_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e165d39-024b-4dd5-afcc-9cb65051c414",
   "metadata": {},
   "source": [
    "**Step 2: Simple demonstration of Input-Target Pairs**\n",
    "\n",
    "Creating 2 variables x & y where x=input & y=target\n",
    "\n",
    "target is always context+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6d52018e-0a9f-4cb4-8014-eb79820b46cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = enc_text[:100]\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a72406e6-9591-48f1-afed-ddd0453b6ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input -> [2025, 39138, 656, 262] Output -> [39138, 656, 262, 10362]\n",
      "Input -> [39138, 656, 262, 10362] Output -> [656, 262, 10362, 290]\n",
      "Input -> [656, 262, 10362, 290] Output -> [262, 10362, 290, 46865]\n",
      "Input -> [262, 10362, 290, 46865] Output -> [10362, 290, 46865, 286]\n",
      "Input -> [10362, 290, 46865, 286] Output -> [290, 46865, 286, 262]\n",
      "Input -> [290, 46865, 286, 262] Output -> [46865, 286, 262, 35151]\n",
      "Input -> [46865, 286, 262, 35151] Output -> [286, 262, 35151, 286]\n",
      "Input -> [286, 262, 35151, 286] Output -> [262, 35151, 286, 7973]\n",
      "Input -> [262, 35151, 286, 7973] Output -> [35151, 286, 7973, 416]\n",
      "Input -> [35151, 286, 7973, 416] Output -> [286, 7973, 416, 7244]\n",
      "Input -> [286, 7973, 416, 7244] Output -> [7973, 416, 7244, 4176]\n",
      "Input -> [7973, 416, 7244, 4176] Output -> [416, 7244, 4176, 318]\n",
      "Input -> [416, 7244, 4176, 318] Output -> [7244, 4176, 318, 257]\n",
      "Input -> [7244, 4176, 318, 257] Output -> [4176, 318, 257, 9207]\n",
      "Input -> [4176, 318, 257, 9207] Output -> [318, 257, 9207, 286]\n",
      "Input -> [318, 257, 9207, 286] Output -> [257, 9207, 286, 383]\n",
      "Input -> [257, 9207, 286, 383] Output -> [9207, 286, 383, 198]\n",
      "Input -> [9207, 286, 383, 198] Output -> [286, 383, 198, 19453]\n",
      "Input -> [286, 383, 198, 19453] Output -> [383, 198, 19453, 4565]\n",
      "Input -> [383, 198, 19453, 4565] Output -> [198, 19453, 4565, 40184]\n",
      "Input -> [198, 19453, 4565, 40184] Output -> [19453, 4565, 40184, 7171]\n",
      "Input -> [19453, 4565, 40184, 7171] Output -> [4565, 40184, 7171, 13]\n",
      "Input -> [4565, 40184, 7171, 13] Output -> [40184, 7171, 13, 770]\n",
      "Input -> [40184, 7171, 13, 770] Output -> [7171, 13, 770, 44685]\n",
      "Input -> [7171, 13, 770, 44685] Output -> [13, 770, 44685, 16854]\n",
      "Input -> [13, 770, 44685, 16854] Output -> [770, 44685, 16854, 2393]\n",
      "Input -> [770, 44685, 16854, 2393] Output -> [44685, 16854, 2393, 318]\n",
      "Input -> [44685, 16854, 2393, 318] Output -> [16854, 2393, 318, 30760]\n",
      "Input -> [16854, 2393, 318, 30760] Output -> [2393, 318, 30760, 1479]\n",
      "Input -> [2393, 318, 30760, 1479] Output -> [318, 30760, 1479, 290]\n",
      "Input -> [318, 30760, 1479, 290] Output -> [30760, 1479, 290, 1231]\n",
      "Input -> [30760, 1479, 290, 1231] Output -> [1479, 290, 1231, 597]\n",
      "Input -> [1479, 290, 1231, 597] Output -> [290, 1231, 597, 3877]\n",
      "Input -> [290, 1231, 597, 3877] Output -> [1231, 597, 3877, 286]\n",
      "Input -> [1231, 597, 3877, 286] Output -> [597, 3877, 286, 597]\n",
      "Input -> [597, 3877, 286, 597] Output -> [3877, 286, 597, 198]\n",
      "Input -> [3877, 286, 597, 198] Output -> [286, 597, 198, 11031]\n",
      "Input -> [286, 597, 198, 11031] Output -> [597, 198, 11031, 13]\n",
      "Input -> [597, 198, 11031, 13] Output -> [198, 11031, 13, 4377]\n",
      "Input -> [198, 11031, 13, 4377] Output -> [11031, 13, 4377, 1048]\n",
      "Input -> [11031, 13, 4377, 1048] Output -> [13, 4377, 1048, 1262]\n",
      "Input -> [13, 4377, 1048, 1262] Output -> [4377, 1048, 1262, 428]\n",
      "Input -> [4377, 1048, 1262, 428] Output -> [1048, 1262, 428, 3188]\n",
      "Input -> [1048, 1262, 428, 3188] Output -> [1262, 428, 3188, 2393]\n",
      "Input -> [1262, 428, 3188, 2393] Output -> [428, 3188, 2393, 11]\n",
      "Input -> [428, 3188, 2393, 11] Output -> [3188, 2393, 11, 329]\n",
      "Input -> [3188, 2393, 11, 329] Output -> [2393, 11, 329, 597]\n",
      "Input -> [2393, 11, 329, 597] Output -> [11, 329, 597, 4007]\n",
      "Input -> [11, 329, 597, 4007] Output -> [329, 597, 4007, 11]\n",
      "Input -> [329, 597, 4007, 11] Output -> [597, 4007, 11, 290]\n",
      "Input -> [597, 4007, 11, 290] Output -> [4007, 11, 290, 287]\n",
      "Input -> [4007, 11, 290, 287] Output -> [11, 290, 287, 597]\n",
      "Input -> [11, 290, 287, 597] Output -> [290, 287, 597, 835]\n",
      "Input -> [290, 287, 597, 835] Output -> [287, 597, 835, 857]\n",
      "Input -> [287, 597, 835, 857] Output -> [597, 835, 857, 523]\n",
      "Input -> [597, 835, 857, 523] Output -> [835, 857, 523, 379]\n",
      "Input -> [835, 857, 523, 379] Output -> [857, 523, 379, 465]\n",
      "Input -> [857, 523, 379, 465] Output -> [523, 379, 465, 393]\n",
      "Input -> [523, 379, 465, 393] Output -> [379, 465, 393, 607]\n",
      "Input -> [379, 465, 393, 607] Output -> [465, 393, 607, 898]\n",
      "Input -> [465, 393, 607, 898] Output -> [393, 607, 898, 198]\n",
      "Input -> [393, 607, 898, 198] Output -> [607, 898, 198, 19121]\n",
      "Input -> [607, 898, 198, 19121] Output -> [898, 198, 19121, 13]\n",
      "Input -> [898, 198, 19121, 13] Output -> [198, 19121, 13, 16126]\n",
      "Input -> [198, 19121, 13, 16126] Output -> [19121, 13, 16126, 262]\n",
      "Input -> [19121, 13, 16126, 262] Output -> [13, 16126, 262, 9589]\n",
      "Input -> [13, 16126, 262, 9589] Output -> [16126, 262, 9589, 1812]\n",
      "Input -> [16126, 262, 9589, 1812] Output -> [262, 9589, 1812, 2059]\n",
      "Input -> [262, 9589, 1812, 2059] Output -> [9589, 1812, 2059, 4249]\n",
      "Input -> [9589, 1812, 2059, 4249] Output -> [1812, 2059, 4249, 5395]\n",
      "Input -> [1812, 2059, 4249, 5395] Output -> [2059, 4249, 5395, 1869]\n",
      "Input -> [2059, 4249, 5395, 1869] Output -> [4249, 5395, 1869, 271]\n",
      "Input -> [4249, 5395, 1869, 271] Output -> [5395, 1869, 271, 11]\n",
      "Input -> [5395, 1869, 271, 11] Output -> [1869, 271, 11, 12058]\n",
      "Input -> [1869, 271, 11, 12058] Output -> [271, 11, 12058, 11]\n",
      "Input -> [271, 11, 12058, 11] Output -> [11, 12058, 11, 4249]\n",
      "Input -> [11, 12058, 11, 4249] Output -> [12058, 11, 4249, 2687]\n",
      "Input -> [12058, 11, 4249, 2687] Output -> [11, 4249, 2687, 3917]\n",
      "Input -> [11, 4249, 2687, 3917] Output -> [4249, 2687, 3917, 351]\n",
      "Input -> [4249, 2687, 3917, 351] Output -> [2687, 3917, 351, 262]\n",
      "Input -> [2687, 3917, 351, 262] Output -> [3917, 351, 262, 198]\n",
      "Input -> [3917, 351, 262, 198] Output -> [351, 262, 198, 39899]\n",
      "Input -> [351, 262, 198, 39899] Output -> [262, 198, 39899, 9270]\n",
      "Input -> [262, 198, 39899, 9270] Output -> [198, 39899, 9270, 1812]\n",
      "Input -> [198, 39899, 9270, 1812] Output -> [39899, 9270, 1812, 2059]\n",
      "Input -> [39899, 9270, 1812, 2059] Output -> [9270, 1812, 2059, 18533]\n",
      "Input -> [9270, 1812, 2059, 18533] Output -> [1812, 2059, 18533, 597]\n",
      "Input -> [1812, 2059, 18533, 597] Output -> [2059, 18533, 597, 5798]\n",
      "Input -> [2059, 18533, 597, 5798] Output -> [18533, 597, 5798, 329]\n",
      "Input -> [18533, 597, 5798, 329] Output -> [597, 5798, 329, 262]\n",
      "Input -> [597, 5798, 329, 262] Output -> [5798, 329, 262, 2587]\n",
      "Input -> [5798, 329, 262, 2587] Output -> [329, 262, 2587, 7763]\n",
      "Input -> [329, 262, 2587, 7763] Output -> [262, 2587, 7763, 1626]\n",
      "Input -> [262, 2587, 7763, 1626] Output -> [2587, 7763, 1626, 262]\n",
      "Input -> [2587, 7763, 1626, 262] Output -> [7763, 1626, 262, 3188]\n",
      "Input -> [7763, 1626, 262, 3188] Output -> [1626, 262, 3188, 198]\n"
     ]
    }
   ],
   "source": [
    "# context size for the input\n",
    "context_size = 4\n",
    "# creating input and output variables\n",
    "xs, ys = [], []\n",
    "for i in range(len(sample)-context_size):\n",
    "    x = enc_text[i:i+context_size]\n",
    "    xs.append(x)\n",
    "    y = enc_text[i+1:i+context_size+1]\n",
    "    ys.append(y)\n",
    "    print(f'Input -> {x} Output -> {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d9f6148-5a65-4f39-96dd-f0dd153227e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input -> An Inquiry into the Output ->  Inquiry into the Nature\n",
      "Input ->  Inquiry into the Nature Output ->  into the Nature and\n",
      "Input ->  into the Nature and Output ->  the Nature and Causes\n",
      "Input ->  the Nature and Causes Output ->  Nature and Causes of\n",
      "Input ->  Nature and Causes of Output ->  and Causes of the\n",
      "Input ->  and Causes of the Output ->  Causes of the Wealth\n",
      "Input ->  Causes of the Wealth Output ->  of the Wealth of\n",
      "Input ->  of the Wealth of Output ->  the Wealth of Nations\n",
      "Input ->  the Wealth of Nations Output ->  Wealth of Nations by\n",
      "Input ->  Wealth of Nations by Output ->  of Nations by Adam\n",
      "Input ->  of Nations by Adam Output ->  Nations by Adam Smith\n",
      "Input ->  Nations by Adam Smith Output ->  by Adam Smith is\n",
      "Input ->  by Adam Smith is Output ->  Adam Smith is a\n",
      "Input ->  Adam Smith is a Output ->  Smith is a publication\n",
      "Input ->  Smith is a publication Output ->  is a publication of\n",
      "Input ->  is a publication of Output ->  a publication of The\n",
      "Input ->  a publication of The Output ->  publication of The\n",
      "\n",
      "Input ->  publication of The\n",
      " Output ->  of The\n",
      "Elect\n",
      "Input ->  of The\n",
      "Elect Output ->  The\n",
      "Electronic\n",
      "Input ->  The\n",
      "Electronic Output -> \n",
      "Electronic Classics\n",
      "Input -> \n",
      "Electronic Classics Output -> Electronic Classics Series\n",
      "Input -> Electronic Classics Series Output -> ronic Classics Series.\n",
      "Input -> ronic Classics Series. Output ->  Classics Series. This\n",
      "Input ->  Classics Series. This Output ->  Series. This Portable\n",
      "Input ->  Series. This Portable Output -> . This Portable Document\n",
      "Input -> . This Portable Document Output ->  This Portable Document file\n",
      "Input ->  This Portable Document file Output ->  Portable Document file is\n",
      "Input ->  Portable Document file is Output ->  Document file is furnished\n",
      "Input ->  Document file is furnished Output ->  file is furnished free\n",
      "Input ->  file is furnished free Output ->  is furnished free and\n",
      "Input ->  is furnished free and Output ->  furnished free and without\n",
      "Input ->  furnished free and without Output ->  free and without any\n",
      "Input ->  free and without any Output ->  and without any charge\n",
      "Input ->  and without any charge Output ->  without any charge of\n",
      "Input ->  without any charge of Output ->  any charge of any\n",
      "Input ->  any charge of any Output ->  charge of any\n",
      "\n",
      "Input ->  charge of any\n",
      " Output ->  of any\n",
      "kind\n",
      "Input ->  of any\n",
      "kind Output ->  any\n",
      "kind.\n",
      "Input ->  any\n",
      "kind. Output -> \n",
      "kind. Any\n",
      "Input -> \n",
      "kind. Any Output -> kind. Any person\n",
      "Input -> kind. Any person Output -> . Any person using\n",
      "Input -> . Any person using Output ->  Any person using this\n",
      "Input ->  Any person using this Output ->  person using this document\n",
      "Input ->  person using this document Output ->  using this document file\n",
      "Input ->  using this document file Output ->  this document file,\n",
      "Input ->  this document file, Output ->  document file, for\n",
      "Input ->  document file, for Output ->  file, for any\n",
      "Input ->  file, for any Output -> , for any purpose\n",
      "Input -> , for any purpose Output ->  for any purpose,\n",
      "Input ->  for any purpose, Output ->  any purpose, and\n",
      "Input ->  any purpose, and Output ->  purpose, and in\n",
      "Input ->  purpose, and in Output -> , and in any\n",
      "Input -> , and in any Output ->  and in any way\n",
      "Input ->  and in any way Output ->  in any way does\n",
      "Input ->  in any way does Output ->  any way does so\n",
      "Input ->  any way does so Output ->  way does so at\n",
      "Input ->  way does so at Output ->  does so at his\n",
      "Input ->  does so at his Output ->  so at his or\n",
      "Input ->  so at his or Output ->  at his or her\n",
      "Input ->  at his or her Output ->  his or her own\n",
      "Input ->  his or her own Output ->  or her own\n",
      "\n",
      "Input ->  or her own\n",
      " Output ->  her own\n",
      "risk\n",
      "Input ->  her own\n",
      "risk Output ->  own\n",
      "risk.\n",
      "Input ->  own\n",
      "risk. Output -> \n",
      "risk. Neither\n",
      "Input -> \n",
      "risk. Neither Output -> risk. Neither the\n",
      "Input -> risk. Neither the Output -> . Neither the Pennsylvania\n",
      "Input -> . Neither the Pennsylvania Output ->  Neither the Pennsylvania State\n",
      "Input ->  Neither the Pennsylvania State Output ->  the Pennsylvania State University\n",
      "Input ->  the Pennsylvania State University Output ->  Pennsylvania State University nor\n",
      "Input ->  Pennsylvania State University nor Output ->  State University nor Jim\n",
      "Input ->  State University nor Jim Output ->  University nor Jim Man\n",
      "Input ->  University nor Jim Man Output ->  nor Jim Manis\n",
      "Input ->  nor Jim Manis Output ->  Jim Manis,\n",
      "Input ->  Jim Manis, Output ->  Manis, Editor\n",
      "Input ->  Manis, Editor Output -> is, Editor,\n",
      "Input -> is, Editor, Output -> , Editor, nor\n",
      "Input -> , Editor, nor Output ->  Editor, nor anyone\n",
      "Input ->  Editor, nor anyone Output -> , nor anyone associated\n",
      "Input -> , nor anyone associated Output ->  nor anyone associated with\n",
      "Input ->  nor anyone associated with Output ->  anyone associated with the\n",
      "Input ->  anyone associated with the Output ->  associated with the\n",
      "\n",
      "Input ->  associated with the\n",
      " Output ->  with the\n",
      "Penn\n",
      "Input ->  with the\n",
      "Penn Output ->  the\n",
      "Pennsylvania\n",
      "Input ->  the\n",
      "Pennsylvania Output -> \n",
      "Pennsylvania State\n",
      "Input -> \n",
      "Pennsylvania State Output -> Pennsylvania State University\n",
      "Input -> Pennsylvania State University Output -> sylvania State University assumes\n",
      "Input -> sylvania State University assumes Output ->  State University assumes any\n",
      "Input ->  State University assumes any Output ->  University assumes any responsibility\n",
      "Input ->  University assumes any responsibility Output ->  assumes any responsibility for\n",
      "Input ->  assumes any responsibility for Output ->  any responsibility for the\n",
      "Input ->  any responsibility for the Output ->  responsibility for the material\n",
      "Input ->  responsibility for the material Output ->  for the material contained\n",
      "Input ->  for the material contained Output ->  the material contained within\n",
      "Input ->  the material contained within Output ->  material contained within the\n",
      "Input ->  material contained within the Output ->  contained within the document\n",
      "Input ->  contained within the document Output ->  within the document\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decoding\n",
    "for i in range(len(sample)-context_size):\n",
    "    x = tokenizer.decode(enc_text[i:i+context_size])\n",
    "    xs.append(x)\n",
    "    y = tokenizer.decode(enc_text[i+1:i+context_size+1])\n",
    "    ys.append(y)\n",
    "    print(f'Input -> {x} Output -> {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a43b0e-cd16-4b05-9908-2032fcfad0f4",
   "metadata": {},
   "source": [
    "**Step 2: Implementing Dataloader**\n",
    "\n",
    "*Dataloader* -> loading & processing data - functionalities for batching, shuffling, and processing data. In PyTorch, DataLoader is a utility that helps you efficiently load your dataset in mini-batches, shuffle it, and use multiple workers to speed up training.\n",
    "\n",
    "First, we tokenize the text and create input-target chunks.\n",
    "\n",
    "GPTDataset defines how inputs are fetched from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4faa7646-fc6e-418c-b12f-4270d61843ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "248f9855-fff1-44b9-8f3a-def04f758754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    #for training a GPT-like model\n",
    "    def __init__(self, raw_text, tokenizer,maxlength, stride):\n",
    "        #maxlength here, is like the context_size used in creating simple input-target pairs\n",
    "        #stride -> controls how much the sliding window moves when new input-target pair is created\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "\n",
    "        #tokenizing the text\n",
    "        token_ids = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "            #BPE tokenizer used\n",
    "        #using sliding window to chunk input and target\n",
    "        for i in range(0,len(token_ids) - maxlength, stride): #step value = stride\n",
    "            input_chunk = token_ids[i:i+maxlength]\n",
    "            target_chunk = token_ids[i+1:i+maxlength+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    # fucntion for returning the length of input ids\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    #fucntion to get input-target ids at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bb4ed-be87-4996-a8aa-24288fc5ca04",
   "metadata": {},
   "source": [
    "Implementing DataLoader\n",
    "\n",
    "We have high value for stride to avoid overlapping. \n",
    "\n",
    "Less stride leads to more overlapping can lead to overfitting for predicting next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "40ed96e4-7215-4e0d-8020-50283a96ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(raw_text, batch_size=4, maxlength=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # batch_size= How many samples the model processes at once before updating weights\n",
    "    #drop_last = drops the last incomplete batch\n",
    "    #num_workers=How many subprocesses to use for loading data in parallel\n",
    "\n",
    "    #creating a tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    #created dataset\n",
    "    dataset=GPTDataset(raw_text, tokenizer, maxlength, stride)\n",
    "\n",
    "    #creating dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last,num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33269126-2fd7-4ddf-9507-9a3bfa94525a",
   "metadata": {},
   "source": [
    "**Testing the dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c3118f38-b244-4631-acd4-8487e735ff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 2025, 39138,   656,   262, 10362,   290, 46865,   286],\n",
      "        [10362,   290, 46865,   286,   262, 35151,   286,  7973],\n",
      "        [  262, 35151,   286,  7973,   416,  7244,  4176,   318],\n",
      "        [  416,  7244,  4176,   318,   257,  9207,   286,   383]]), tensor([[39138,   656,   262, 10362,   290, 46865,   286,   262],\n",
      "        [  290, 46865,   286,   262, 35151,   286,  7973,   416],\n",
      "        [35151,   286,  7973,   416,  7244,  4176,   318,   257],\n",
      "        [ 7244,  4176,   318,   257,  9207,   286,   383,   198]])]\n"
     ]
    }
   ],
   "source": [
    "# creating a dataloader\n",
    "\n",
    "dataloader = data_loader(raw_text, batch_size=4, maxlength=8, stride=4, shuffle=False)\n",
    "\n",
    "#iterating over the data\n",
    "data_iter = iter(dataloader)\n",
    "batch_n = next(data_iter)\n",
    "\n",
    "print(batch_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a81969-fe6e-4bba-9777-b873888e8c29",
   "metadata": {},
   "source": [
    "Thus, it indicates that - iterate over the data in *4 batches* with *context_size=8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc5cb2-a4a2-4f13-8cdc-7fc5c1df04eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
