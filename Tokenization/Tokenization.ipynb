{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3098ea3-cdfe-46ca-acae-14208d6b9511",
   "metadata": {},
   "source": [
    "# **TOKENIZATION**\n",
    "# What is Tokenization? \n",
    "# Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking down a stream of text into smaller units called tokens.\n",
    "(Source: GeeksforGeeks)\n",
    "\n",
    "\n",
    "*Data (Book) : The Wealth of Nations by Adam Smith*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c613295-d207-49d0-9726-e2b765be3bf5",
   "metadata": {},
   "source": [
    "**Loading & Reading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "98fca64a-4873-41c1-b2c7-ce5a7788d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('The-Wealth-of-Nations.txt', encoding='utf-8') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f8c1073b-16e4-49e0-8e05-910084bdf32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2248172\n"
     ]
    }
   ],
   "source": [
    "# Number of characters in the text\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d87b82-4493-4f1a-bffa-534787f41d96",
   "metadata": {},
   "source": [
    "The data has more than 22 Lakh characters but not all characters are unique in this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17050d-671b-465a-8ed6-860d38bc5bdf",
   "metadata": {},
   "source": [
    "**Using the Regular Expressions to split the data using patterns. We have taken a book data which has punctuations. We keep the punctuations as the context of a text changes because of a punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3a96bdd7-4a7c-4496-90e5-6753e33baf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the regular expression library i.e re\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "83dd26de-f79b-4ed1-98ed-1c1e6a80d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the text data using the patterns using the re library\n",
    "preprocessed = re.split(r'([,.!:;?_\"()\\']|--|\\s)', raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "da053a03-dfa0-496b-b897-fc21879920b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " ' ',\n",
       " 'Inquiry',\n",
       " ' ',\n",
       " 'into',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Nature',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'Causes',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Wealth',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'Nations',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'Adam',\n",
       " ' ',\n",
       " 'Smith',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'publication',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'The',\n",
       " '\\n',\n",
       " 'Electronic',\n",
       " ' ',\n",
       " 'Classics',\n",
       " ' ',\n",
       " 'Series',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'This',\n",
       " ' ',\n",
       " 'Portable',\n",
       " ' ',\n",
       " 'Document',\n",
       " ' ',\n",
       " 'file',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'furnished',\n",
       " ' ',\n",
       " 'free',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'without',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'charge',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'any',\n",
       " '\\n',\n",
       " 'kind',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'Any',\n",
       " ' ',\n",
       " 'person',\n",
       " ' ',\n",
       " 'using',\n",
       " ' ',\n",
       " 'this',\n",
       " ' ',\n",
       " 'document',\n",
       " ' ',\n",
       " 'file',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'for',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'purpose',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'way',\n",
       " ' ',\n",
       " 'does',\n",
       " ' ',\n",
       " 'so',\n",
       " ' ',\n",
       " 'at',\n",
       " ' ',\n",
       " 'his',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'her',\n",
       " ' ',\n",
       " 'own',\n",
       " '\\n',\n",
       " 'risk',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'Neither',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Pennsylvania',\n",
       " ' ',\n",
       " 'State',\n",
       " ' ',\n",
       " 'University',\n",
       " ' ',\n",
       " 'nor',\n",
       " ' ',\n",
       " 'Jim',\n",
       " ' ',\n",
       " 'Manis',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'Editor',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'nor',\n",
       " ' ',\n",
       " 'anyone',\n",
       " ' ',\n",
       " 'associated',\n",
       " ' ',\n",
       " 'with',\n",
       " ' ',\n",
       " 'the',\n",
       " '\\n',\n",
       " 'Pennsylvania',\n",
       " ' ',\n",
       " 'State',\n",
       " ' ',\n",
       " 'University',\n",
       " ' ',\n",
       " 'assumes',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'responsibility',\n",
       " ' ',\n",
       " 'for',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'material',\n",
       " ' ',\n",
       " 'contained',\n",
       " ' ',\n",
       " 'within',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'document',\n",
       " '\\n',\n",
       " 'or',\n",
       " ' ',\n",
       " 'for',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'file',\n",
       " ' ',\n",
       " 'as',\n",
       " ' ',\n",
       " 'an',\n",
       " ' ',\n",
       " 'electronic',\n",
       " ' ',\n",
       " 'transmission',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'way',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'An',\n",
       " ' ',\n",
       " 'Inquiry',\n",
       " ' ',\n",
       " 'into',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Nature',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'Causes',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Wealth',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'Nations',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'Adam',\n",
       " ' ',\n",
       " 'Smith',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'The',\n",
       " ' ',\n",
       " 'Electronic',\n",
       " ' ',\n",
       " 'Classics',\n",
       " '\\n',\n",
       " 'Series',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'Jim',\n",
       " ' ',\n",
       " 'Manis',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'Editor',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'PSU-Hazleton',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'Hazleton',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'PA',\n",
       " ' ',\n",
       " '18202',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'Portable',\n",
       " ' ',\n",
       " 'Document',\n",
       " ' ',\n",
       " 'File',\n",
       " ' ',\n",
       " 'produced',\n",
       " ' ',\n",
       " 'as',\n",
       " '\\n',\n",
       " 'part',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'an',\n",
       " ' ',\n",
       " 'ongoing',\n",
       " ' ',\n",
       " 'publication',\n",
       " ' ',\n",
       " 'project',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'bring',\n",
       " ' ',\n",
       " 'classical',\n",
       " ' ',\n",
       " 'works',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'literature',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'English',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'free',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'easy',\n",
       " '\\n',\n",
       " 'access',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'those',\n",
       " ' ',\n",
       " 'wishing',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'make',\n",
       " ' ',\n",
       " 'use',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'them',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'Jim',\n",
       " ' ',\n",
       " 'Manis',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'faculty',\n",
       " ' ',\n",
       " 'member',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'English',\n",
       " ' ',\n",
       " 'Department',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'The',\n",
       " ' ',\n",
       " 'Pennsylvania',\n",
       " ' ',\n",
       " 'State',\n",
       " ' ',\n",
       " 'University',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'This',\n",
       " ' ',\n",
       " 'page',\n",
       " '\\n',\n",
       " 'and',\n",
       " ' ',\n",
       " 'any',\n",
       " ' ',\n",
       " 'preceding',\n",
       " ' ',\n",
       " 'page',\n",
       " '(',\n",
       " 's',\n",
       " ')',\n",
       " '',\n",
       " ' ',\n",
       " 'are',\n",
       " ' ',\n",
       " 'restricted',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'copyright',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'The',\n",
       " ' ',\n",
       " 'text',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'following',\n",
       " ' ',\n",
       " 'pages',\n",
       " ' ',\n",
       " 'are',\n",
       " ' ',\n",
       " 'not',\n",
       " ' ',\n",
       " 'copyrighted',\n",
       " '\\n',\n",
       " 'within',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'United',\n",
       " ' ',\n",
       " 'States',\n",
       " ';',\n",
       " '',\n",
       " ' ',\n",
       " 'however',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'fonts',\n",
       " ' ',\n",
       " 'used',\n",
       " ' ',\n",
       " 'may',\n",
       " ' ',\n",
       " 'be',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'Cover',\n",
       " ' ',\n",
       " 'Design',\n",
       " ':',\n",
       " '',\n",
       " ' ',\n",
       " 'Jim',\n",
       " ' ',\n",
       " 'Manis',\n",
       " '\\n',\n",
       " 'Copyright',\n",
       " ' ',\n",
       " '©',\n",
       " ' ',\n",
       " '2005',\n",
       " '\\n',\n",
       " 'The',\n",
       " ' ',\n",
       " 'Pennsylvania',\n",
       " ' ',\n",
       " 'State',\n",
       " ' ',\n",
       " 'University',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'an',\n",
       " ' ',\n",
       " 'equal',\n",
       " ' ',\n",
       " 'opportunity',\n",
       " ' ',\n",
       " 'university',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'I',\n",
       " ' ',\n",
       " 'have',\n",
       " ' ',\n",
       " 'endeavoured',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'fourth',\n",
       " ' ',\n",
       " 'book',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'explain',\n",
       " ' ',\n",
       " 'as',\n",
       " ' ',\n",
       " 'fully',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'distinctly',\n",
       " ' ',\n",
       " 'as',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'can',\n",
       " ' ',\n",
       " 'those',\n",
       " ' ',\n",
       " 'different',\n",
       " ' ',\n",
       " 'theories',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'principal',\n",
       " ' ',\n",
       " 'effects',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'CAUSES',\n",
       " ' ',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'IMPROVEMENT',\n",
       " ' ',\n",
       " 'IN',\n",
       " '\\n',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'PRODUCTIVE',\n",
       " ' ',\n",
       " 'POWERS',\n",
       " ' ',\n",
       " 'OF',\n",
       " '\\n',\n",
       " 'L',\n",
       " ' ',\n",
       " 'ABOUR',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'AND',\n",
       " ' ',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'ORDER',\n",
       " ' ',\n",
       " 'ACCORDING',\n",
       " ' ',\n",
       " 'TO',\n",
       " ' ',\n",
       " 'WHICH',\n",
       " ' ',\n",
       " 'ITS',\n",
       " ' ',\n",
       " 'PRODUCE',\n",
       " ' ',\n",
       " 'IS',\n",
       " '\\n',\n",
       " 'NATURALLY',\n",
       " ' ',\n",
       " 'DISTRIBUTED',\n",
       " ' ',\n",
       " 'AMONG',\n",
       " '\\n',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'DIFFERENT',\n",
       " ' ',\n",
       " 'RANKS',\n",
       " ' ',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'THE',\n",
       " '\\n',\n",
       " 'PEOPLE',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'which',\n",
       " ' ',\n",
       " 'they',\n",
       " ' ',\n",
       " 'have',\n",
       " ' ',\n",
       " 'produced',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'different',\n",
       " ' ',\n",
       " 'ages',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'nations',\n",
       " '.',\n",
       " '',\n",
       " '\\n',\n",
       " 'To',\n",
       " ' ',\n",
       " 'explain',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'what',\n",
       " ' ',\n",
       " 'has',\n",
       " ' ',\n",
       " 'consisted',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'revenue',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'great',\n",
       " ' ',\n",
       " 'body',\n",
       " '\\n',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'people',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'what',\n",
       " ' ',\n",
       " 'has',\n",
       " ' ',\n",
       " 'been',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'nature',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'those',\n",
       " ' ',\n",
       " 'funds',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'which',\n",
       " ',',\n",
       " '',\n",
       " '\\n',\n",
       " 'in',\n",
       " ' ',\n",
       " 'different',\n",
       " ' ',\n",
       " 'ages',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'nations',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'have',\n",
       " ' ',\n",
       " 'supplied',\n",
       " ' ',\n",
       " 'their',\n",
       " ' ',\n",
       " 'annual',\n",
       " ' ',\n",
       " 'consumption',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'object',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'these',\n",
       " ' ',\n",
       " 'four',\n",
       " ' ',\n",
       " 'first',\n",
       " ' ',\n",
       " 'books',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'The',\n",
       " ' ',\n",
       " 'fifth',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'last',\n",
       " ' ',\n",
       " 'book',\n",
       " '\\n',\n",
       " 'treats',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'revenue',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'sovereign',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'commonwealth',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'In',\n",
       " ' ',\n",
       " 'this',\n",
       " '\\n',\n",
       " 'book',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'have',\n",
       " ' ',\n",
       " 'endeavoured',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'shew',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'first',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'what',\n",
       " ' ',\n",
       " 'are',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'necessary',\n",
       " '\\n',\n",
       " 'expenses',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'sovereign',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'commonwealth',\n",
       " ';',\n",
       " '',\n",
       " ' ',\n",
       " 'which',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'those',\n",
       " ' ',\n",
       " 'ex-',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'CHAPTER',\n",
       " ' ',\n",
       " 'I',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'penses',\n",
       " ' ',\n",
       " 'ought',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'be',\n",
       " ' ',\n",
       " 'defrayed',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'general',\n",
       " ' ',\n",
       " 'contribution',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " '\\n',\n",
       " 'whole',\n",
       " ' ',\n",
       " 'society',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'which',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'them',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'by',\n",
       " ' ',\n",
       " 'that',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'some',\n",
       " ' ',\n",
       " 'particular',\n",
       " ' ',\n",
       " 'part',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'THE',\n",
       " ' ',\n",
       " 'DIVISION',\n",
       " ' ',\n",
       " 'OF',\n",
       " ' ',\n",
       " 'LABOUR',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'only',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'or',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'some',\n",
       " ' ',\n",
       " 'particular',\n",
       " ' ',\n",
       " 'members',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'it',\n",
       " ':',\n",
       " '',\n",
       " ' ',\n",
       " 'secondly',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'what',\n",
       " ' ',\n",
       " 'are',\n",
       " ' ',\n",
       " 'the',\n",
       " '\\n',\n",
       " 'different',\n",
       " ' ',\n",
       " 'methods',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'which',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'whole',\n",
       " ' ',\n",
       " 'society',\n",
       " ' ',\n",
       " 'may',\n",
       " ' ',\n",
       " 'be',\n",
       " ' ',\n",
       " 'made',\n",
       " ' ',\n",
       " 'to',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'T',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'HE',\n",
       " ' ',\n",
       " 'GREATEST',\n",
       " ' ',\n",
       " 'IMPROVEMENTS',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'productive',\n",
       " ' ',\n",
       " 'powers',\n",
       " ' ',\n",
       " 'of',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n',\n",
       " 'labour',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'greater',\n",
       " ' ',\n",
       " 'part',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'skill',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'dexterity',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'and',\n",
       " '\\n',\n",
       " 'judgment',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'with',\n",
       " ' ',\n",
       " 'which',\n",
       " ' ',\n",
       " 'it',\n",
       " ' ',\n",
       " 'is',\n",
       " ' ',\n",
       " 'anywhere',\n",
       " ' ',\n",
       " ...]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d389a4-29ac-4f5e-bf8d-af8de75bdb4b",
   "metadata": {},
   "source": [
    "From the above output, we can see that it contains white spaces too!\n",
    "\n",
    "We want to remove the white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "909bc53f-ddbe-4fd3-9308-8f2ac9e0a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = [item for item in preprocessed if item.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9292d01-3263-4e07-a324-6446ae0aa445",
   "metadata": {},
   "source": [
    "Tokens i.e words have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "aa3dd76f-8b02-4ebb-b969-33a11284ed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " 'Inquiry',\n",
       " 'into',\n",
       " 'the',\n",
       " 'Nature',\n",
       " 'and',\n",
       " 'Causes',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Wealth',\n",
       " 'of',\n",
       " 'Nations',\n",
       " 'by',\n",
       " 'Adam',\n",
       " 'Smith',\n",
       " 'is',\n",
       " 'a',\n",
       " 'publication',\n",
       " 'of',\n",
       " 'The']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f8a3e-2991-4e68-bf3f-f66644b8d7be",
   "metadata": {},
   "source": [
    "Thus, we successfully removed the white spaces from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c749cb7-ee4a-49b9-89b8-73ca167d4876",
   "metadata": {},
   "source": [
    "# Vocabulary: In the context of Large Language Models (LLMs), the vocabulary is the set of all tokens (words, subwords, or characters) that the model knows and can understand.\n",
    "\n",
    "We create a vocabulary of the given data. So that when the model is trained and used it knows which words to use.\n",
    "\n",
    "Vocabulary is a dictionary of words with proper IDs.\n",
    "\n",
    "Thus, the words in the vocabulary should be unique and not repeated.\n",
    "\n",
    "\n",
    "**Creating vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "486424a7-06e0-45e8-9f7d-771cfc20885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: get all unique words sorted alphabetically\n",
    "unique_words = sorted(set(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cb60c1dc-0db7-4b9d-b7dd-4d0c560cc321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13984"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(unique_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4bca1-aab9-44be-9bce-1f5f77b30afd",
   "metadata": {},
   "source": [
    "So, from the 22 Lakh words we have only 13,984 unique words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7140c-a18c-41b8-8757-1178b4a86bb9",
   "metadata": {},
   "source": [
    "**Now, we create a *Vocabulary* and assign unique *TokenIDs* to the tokens(here, words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e0af8b03-96c5-476a-935b-6d0e5df83c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c2629-8088-4019-98da-37b7c26ab7e0",
   "metadata": {},
   "source": [
    "We swap the token and integer to create a tokenID (integer) for that specific token (token).\n",
    "\n",
    "*enumerate()* function aloops through all elements and returns both value and index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a155a87-db4b-4384-a294-3820ba528389",
   "metadata": {},
   "source": [
    "A Vocabulary is a dictionary which provides mapping from token to token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b66c448d-f3fb-4994-a67b-4a451a63d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('#', 1)\n",
      "('(', 2)\n",
      "(')', 3)\n",
      "(',', 4)\n",
      "('-', 5)\n",
      "('.', 6)\n",
      "('/', 7)\n",
      "('//www', 8)\n",
      "('0', 9)\n",
      "('000', 10)\n",
      "('001', 11)\n",
      "('017', 12)\n",
      "('023', 13)\n",
      "('027', 14)\n",
      "('029', 15)\n",
      "('041', 16)\n",
      "('055', 17)\n",
      "('068', 18)\n",
      "('075', 19)\n",
      "('076', 20)\n",
      "('083', 21)\n",
      "('086', 22)\n",
      "('092', 23)\n",
      "('0d', 24)\n",
      "('0¹/³', 25)\n",
      "('0¼', 26)\n",
      "('0½', 27)\n",
      "('0¾', 28)\n",
      "('0¾d', 29)\n",
      "('1', 30)\n",
      "('1-4th', 31)\n",
      "('1/', 32)\n",
      "('1/12d', 33)\n",
      "('1/2', 34)\n",
      "('1/3', 35)\n",
      "('1/3d', 36)\n",
      "('1/6', 37)\n",
      "('10', 38)\n",
      "('10/32', 39)\n",
      "('100', 40)\n",
      "('1000', 41)\n",
      "('101', 42)\n",
      "('102', 43)\n",
      "('103', 44)\n",
      "('104', 45)\n",
      "('105', 46)\n",
      "('106', 47)\n",
      "('107', 48)\n",
      "('108', 49)\n"
     ]
    }
   ],
   "source": [
    "# displaying some part of the vocabulary\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    if(i<50):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc617b-65bb-4883-a320-fa695af3055d",
   "metadata": {},
   "source": [
    "# This process can be called as *Encoding*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a95f1-38e4-4654-86e9-101c7b96e472",
   "metadata": {},
   "source": [
    "In simple terms, \n",
    "\n",
    "**Encoding** means turning text (like words or sentences) into numbers (token IDs) that a computer or model can understand.\n",
    "\n",
    "**Decoding** means turning those numbers (token IDs) back into the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c7cef6-4a4d-464a-9f42-6c01cc47ef23",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee286b22-417b-4243-915f-65b0eb424ae0",
   "metadata": {},
   "source": [
    "We will now implement simple tokenization classes to encode & decode the text then, we will move on to *Types of Tokenizations* and *Byte-Pair Encoding*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c98a7-269a-4407-b359-203d8eddf921",
   "metadata": {},
   "source": [
    "# Tokenizer Class 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3feb8d04-cbee-4347-92ab-d6c6a91f0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer1:\n",
    "    def __init__(self, vocab): #init -> automatically called when an instance of a class is created\n",
    "        self.str_to_int = vocab \n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) #tokenized the text\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()] #removing white spaces\n",
    "\n",
    "        #creating token ids \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    # decoding the ids to get string\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) #reversing the method used for vocabulary \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', text) #replaced the punctuations\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a82d1-3b39-41f9-960b-c29818d55079",
   "metadata": {},
   "source": [
    "Testing the class by creating an instance of the Tokenizer1 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d7aa936c-eb07-4019-94a0-c5ce9d09d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e5abd815-0eed-4eb7-8046-1dd4b8b46883",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"countries which enjoy the highest degree of industry and improvement; what is the work of one man, in a rude state of society,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "56c6124b-eb7b-43fc-9f6b-9dc1b7647276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5214, 13635, 6396, 12692, 7749, 5511, 9669, 8157, 3520, 8010, 1291, 13612, 8411, 12692, 13740, 9669, 9706, 8941, 4, 8025, 3092, 11471, 12182, 9669, 12010, 4]\n"
     ]
    }
   ],
   "source": [
    "# ENCODING\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b40cf20e-fa6e-4243-b2ca-a2793cfea832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries which enjoy the highest degree of industry and improvement ; what is the work of one man, in a rude state of society,\n"
     ]
    }
   ],
   "source": [
    "# DECODING THE IDs\n",
    "decode = tokenizer.decode(ids)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35563686-6a1b-4057-9dbf-c7f449a875c1",
   "metadata": {},
   "source": [
    "Thus, our Tokenizer1 class o working correctly.\n",
    "\n",
    "\n",
    "But the problem with this approach is - when an unknown words is passed / a word out of vocabulary is passed, it throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f7d1dc98-d225-467d-bb8c-524d0bdf8964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# we can confirm by running this cell\\n\\ntext = \"Apple mobiles are good\\n# ENCODING\\nids = tokenizer.encode(text)\\nprint(ids)\\n# DECODING THE IDs\\ndecode = tokenizer.decode(ids)\\nprint(decode)\\n'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# we can confirm by running this cell\n",
    "\n",
    "text = \"Apple mobiles are good\n",
    "# ENCODING\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "# DECODING THE IDs\n",
    "decode = tokenizer.decode(ids)\n",
    "print(decode)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1484a2-69cf-4507-87e2-24276e692439",
   "metadata": {},
   "source": [
    "To overcome this problem, we use **Special Context Tokens**\n",
    "\n",
    "**Special Context Tokens** can be used to deal with unknown words i.e <|unk|> token or if data is taken from multiple sources we use <|endoftext|> token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50218c87-3f45-43b2-a852-3716e719fcf7",
   "metadata": {},
   "source": [
    "**We create another Tokenizer class to incorporate these tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11802ed5-5453-4e19-95f1-f05ff7f73d5c",
   "metadata": {},
   "source": [
    "We will update the vocabulary by adding the *Special Context Tokens*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c8852718-7cb4-4cf1-8240-5d86653eef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "#adding the new tokens\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "\n",
    "#giving token IDs to the tokens & creating a vocabulary \n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a9012-6c14-4bce-ae3c-e4944ad961d1",
   "metadata": {},
   "source": [
    "Thus, the special context tokens have been added to the vocabulary.\n",
    "\n",
    "We can verify it by-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a102e72e-d152-4490-a072-eef57dd29f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13986"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510fc37c-3b44-4199-8608-4c393d93f6eb",
   "metadata": {},
   "source": [
    "As seen, prior to adding the special context tokens the vocab size was 13984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e223dc73-f8b4-4120-9d0c-338d3f667187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('”', 13983)\n",
      "('<|endoftext|>', 13984)\n",
      "('<|unk|>', 13985)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-3:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835e605-578c-4059-b627-d2fc26ef6d2c",
   "metadata": {},
   "source": [
    "Thus, the last two tokens are special context tokens.\n",
    "\n",
    "We successfully added the *Special Context Tokens*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e89e9b-eb63-4c5e-9100-507ff8d42887",
   "metadata": {},
   "source": [
    "Now we create the Tokenizer class which will encode depending on the type of token(unknown or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0e37f-4843-4084-ac74-ad134c978f77",
   "metadata": {},
   "source": [
    "# Tokenizer Class 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d7cd554f-b43c-43b4-aec1-9c6aa591f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        #in encoding we will check if the given token belongs to the text data. if it doesn't the token will be <|unk|>\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        #checking if there are unknown words and assigning special context tokens\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        #if the item if in the vocabulary -> return the item else name it as <|unk|>\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids): #code remains the same\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1994f07c-04ce-4195-8810-69eb69b15b0c",
   "metadata": {},
   "source": [
    "Testing the Tokenizer2 class by creating as instance of the Tokenizer2 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "868ae045-fdea-4614-8963-bc8d08d8e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer2(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29726c0b-189f-4da3-b315-cc7e3195e41e",
   "metadata": {},
   "source": [
    "**Testing for <|unk|> token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "96c21e7b-d860-4af1-9e58-5f0b10e938a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Thirdly, and lastly, everybody must be thoughtful(itisunknown) how much labour\n",
    "is facilitated and abridged by the application of proper machinery.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5295e6f9-8650-4872-b60b-8e6b47c5839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded => [2893, 4, 3520, 8594, 4, 6565, 9383, 3957, 13985, 2, 13985, 3, 7846, 9349, 8543, 8411, 6789, 3520, 3152, 4278, 12692, 3613, 9669, 10668, 8895, 6]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(f'Encoded => {ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46824326-cfda-4b24-8db4-73be8c6f0f25",
   "metadata": {},
   "source": [
    "The 13985  token indicates the <|unk|> token.\n",
    "\n",
    "We can decode it as-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dbcd9341-5c1a-47a5-a571-634bc6c2f677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded => Thirdly, and lastly, everybody must be <|unk|>( <|unk|>) how much labour is facilitated and abridged by the application of proper machinery.\n"
     ]
    }
   ],
   "source": [
    "original_text = tokenizer.decode(ids)\n",
    "print(f'Decoded => {original_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721de79-8d68-4ee6-8551-ac52cc20c6f7",
   "metadata": {},
   "source": [
    "**Testing for <|endoftext|> token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "25185a83-346e-40f6-92d5-b8455adadac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"\"\"Thirdly, and lastly, everybody must be sensible how much labour\n",
    "is facilitated and abridged by the application of proper machinery.\"\"\"\n",
    "text_2 = \"\"\"It is unnecessary to give any example.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1d9e7eda-42e1-44dc-9e3b-10aa48781a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thirdly, and lastly, everybody must be sensible how much labour\n",
      "is facilitated and abridged by the application of proper machinery.<|endoftext|>It is unnecessary to give any example.\n"
     ]
    }
   ],
   "source": [
    "text = \"<|endoftext|>\".join((text_1, text_2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e00c8-ff1d-48c6-867e-f7da6de773d5",
   "metadata": {},
   "source": [
    "We considered 2 text sources and joined them using <|endoftext|>\n",
    "\n",
    "In models like GPT, <|endoftext|> tokens are used since the data is taken from various different sources. The <|endoftext|> tokens indicates that the data sources are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6e211b8b-ff73-4af2-9ee5-206b4ef28152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded => [2893, 4, 3520, 8594, 4, 6565, 9383, 3957, 11699, 7846, 9349, 8543, 8411, 6789, 3520, 3152, 4278, 12692, 3613, 9669, 10668, 8895, 6, 13985, 8411, 13226, 12837, 7411, 3571, 6595, 6]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(f'Encoded => {ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "eea3cfd6-f117-4c71-a241-7eb71a7eb817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded => Thirdly, and lastly, everybody must be sensible how much labour is facilitated and abridged by the application of proper machinery. <|unk|> is unnecessary to give any example.\n"
     ]
    }
   ],
   "source": [
    "original_text = tokenizer.decode(ids)\n",
    "print(f'Decoded => {original_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326644a4-0c2f-46a1-8051-ce8b2a2de21e",
   "metadata": {},
   "source": [
    "The token number 13985 indicates the <|endoftext|> token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447b8be-8043-4081-88a0-37846508c813",
   "metadata": {},
   "source": [
    "**Testing without any special tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5d785bf9-2a4d-4474-b07b-8bd7112bb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Thirdly, and lastly, everybody must be sensible how much labour\n",
    "is facilitated and abridged by the application of proper machinery.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1fcb27d6-6081-4551-96b1-8c139d39052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded => [2893, 4, 3520, 8594, 4, 6565, 9383, 3957, 11699, 7846, 9349, 8543, 8411, 6789, 3520, 3152, 4278, 12692, 3613, 9669, 10668, 8895, 6]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(f'Encoded => {ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3093adae-36ed-4b6a-b00c-41643c3e9c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded => Thirdly, and lastly, everybody must be sensible how much labour is facilitated and abridged by the application of proper machinery.\n"
     ]
    }
   ],
   "source": [
    "original_text = tokenizer.decode(ids)\n",
    "print(f'Decoded => {original_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1ccea-05f0-4e42-9b55-2f5154944f6a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baced59e-d289-40a2-a1bd-e42ec1881e57",
   "metadata": {},
   "source": [
    "# Additional Tokens\n",
    "\n",
    "# 1) BOS (Beginning of Sequence) - signifies to LLM where a content begins\n",
    "# 2) EOS (End of Sequence) - useful when concatenating texts at end of texts\n",
    "# 3) PAD (Padding) - to ensure texts in batches have same lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32eae4-3db4-40c0-9fd6-5160cdae7d62",
   "metadata": {},
   "source": [
    "# **Types of Tokenizations:** \n",
    "\n",
    "**1. Word-based - every word in a text is a token but it has OOV (out of vocabulary) problems**\n",
    "\n",
    "**2. Character-based - individual characters (letters) are considered as tokens but the meaning of a word is lost**\n",
    "\n",
    "**3. *Sub Word-based***\n",
    "\n",
    "it follows 2 rules:\n",
    "\n",
    "**Rule1 - don't split frequently used words into sub-words** \n",
    "\n",
    "**Rule2 - split rare words into smaller meaningful sub-words**\n",
    "\n",
    "**(helps model learn the different words with the common root word (eg. token, tokens, tokenizing) - helps model learn to differentiate between different words with same suffix)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b3418-5d4e-465c-ac48-a16738978912",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding(BPE)\n",
    "\n",
    "> Models like **GPT** use this type of encoding. It follows a *Sub-Word-based Tokenization*\n",
    "\n",
    "> In this, the most common words/characters are represented a single token while rare words are broken down into 2 or more tokens\n",
    "\n",
    "> In BPE, the data is checked in sequence and the common pair(byte-pair) that occurs is replaced by a byte (variable) that does NOT occur in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed475bfd-b890-4881-9c50-b962442a9996",
   "metadata": {},
   "source": [
    "# Implementing Byte-Pair Encoding\n",
    "\n",
    "> Tiktoken library is used which is a BPE library (used by OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d9856d2f-05ca-42e5-a6d1-75a3500063a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tiktoken if not installed\n",
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b9830868-0300-4f54-8ca8-782e1268ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737de083-6dda-41c5-8ee5-690975bc7823",
   "metadata": {},
   "source": [
    "Instantiating BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8e6a19b2-db8f-4fd5-9fd9-6cedecf0573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0c2fd-fe4a-4016-9478-63faf49bebd4",
   "metadata": {},
   "source": [
    "We use rules of the GPT-2 model to tokenize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0dfd85a4-5df3-40e1-a285-6b71550c31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"But though this equality of treatment should not be productive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a79f5d-ef17-44a8-a5b9-a589033f0dee",
   "metadata": {},
   "source": [
    "Counting number of tokens this text has-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "51976759-7a0d-444c-9155-6da0362dbecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Token count:\", len(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3340080d-ed22-4254-85a3-b8ecbc3280d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1543, 12751, 12742, 6474, 9669, 12961, 11845, 9554, 3957, 10596]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(f'Token IDs: {ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c67407b3-acdc-441b-a884-6a5e7e478dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: But though this equality of treatment should not be productive\n"
     ]
    }
   ],
   "source": [
    "original_text = tokenizer.decode(ids)\n",
    "print(f'Original Text: {original_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147bb38e-2229-4eb0-afa2-54aa12806caf",
   "metadata": {},
   "source": [
    "Thus, we successfully implemented amd tested the titoken's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be0363-ac62-4f21-8fcc-d7323c7e385c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
